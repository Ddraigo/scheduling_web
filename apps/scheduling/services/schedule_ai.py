"""
LLM AI cho s·∫Øp x·∫øp l·ªãch h·ªçc
"""

import os
import re
import logging
import json
from google import genai
from google.genai import types
from typing import List, Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)


class TokenCounter:
    """Utility class ƒë·ªÉ ƒë·∫øm tokens v√† th·ªëng k√™ s·ª≠ d·ª•ng"""
    
    def __init__(self):
        self.usage_history: List[Dict[str, Any]] = []
        self.total_input_tokens = 0
        self.total_output_tokens = 0
    
    def log_usage(self, prompt_len: int, context_len: int, max_output: int, response_len: int = 0, 
                  timestamp: str = None, model: str = "gemini-2.5-flash"):
        """
        Log th·ªëng k√™ token usage
        
        Args:
            prompt_len: ƒê·ªô d√†i text c·ªßa system instruction + user prompt (chars)
            context_len: ƒê·ªô d√†i d·ªØ li·ªáu context (chars)
            max_output: Max output tokens ƒë∆∞·ª£c request
            response_len: ƒê·ªô d√†i response nh·∫≠n ƒë∆∞·ª£c (chars)
            timestamp: Th·ªùi gian request
            model: Model name
        """
        # ∆Ø·ªõc t√≠nh token count (Google Gemini: ~1 token/4 chars cho text)
        estimated_input_tokens = (prompt_len + context_len) // 4
        estimated_output_tokens = response_len // 4 if response_len > 0 else max_output // 4
        
        usage_entry = {
            'timestamp': timestamp or datetime.now().isoformat(),
            'model': model,
            'prompt_chars': prompt_len,
            'context_chars': context_len,
            'response_chars': response_len,
            'estimated_input_tokens': estimated_input_tokens,
            'estimated_output_tokens': estimated_output_tokens,
            'max_output_tokens': max_output,
            'total_estimated_tokens': estimated_input_tokens + estimated_output_tokens
        }
        
        self.usage_history.append(usage_entry)
        self.total_input_tokens += estimated_input_tokens
        self.total_output_tokens += estimated_output_tokens
        
        return usage_entry
    
    def get_summary(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ t·ªïng h·ª£p"""
        return {
            'total_requests': len(self.usage_history),
            'total_input_tokens': self.total_input_tokens,
            'total_output_tokens': self.total_output_tokens,
            'total_tokens': self.total_input_tokens + self.total_output_tokens,
            'average_input_tokens': self.total_input_tokens // max(1, len(self.usage_history)),
            'average_output_tokens': self.total_output_tokens // max(1, len(self.usage_history)),
            'usage_history': self.usage_history
        }
    
    def export_report(self, filepath: str = None) -> str:
        """Export th·ªëng k√™ ra markdown file"""
        summary = self.get_summary()
        
        report = f"""# üìä LLM Token Usage Report
Generated: {datetime.now().isoformat()}

## Summary Statistics
- **Total Requests**: {summary['total_requests']}
- **Total Input Tokens**: {summary['total_input_tokens']:,}
- **Total Output Tokens**: {summary['total_output_tokens']:,}
- **Total Tokens**: {summary['total_tokens']:,}
- **Average Input Tokens/Request**: {summary['average_input_tokens']:,}
- **Average Output Tokens/Request**: {summary['average_output_tokens']:,}

## Detailed Usage History
| # | Timestamp | Model | Input (chars) | Context (chars) | Response (chars) | Est. Input Tokens | Est. Output Tokens | Total Est. Tokens |
|---|-----------|-------|---------------|-----------------|------------------|-------------------|-------------------|-------------------|
"""
        for i, usage in enumerate(summary['usage_history'], 1):
            report += f"| {i} | {usage['timestamp']} | {usage['model']} | {usage['prompt_chars']:,} | {usage['context_chars']:,} | {usage['response_chars']:,} | {usage['estimated_input_tokens']:,} | {usage['estimated_output_tokens']:,} | {usage['total_estimated_tokens']:,} |\n"
        
        report += f"\n## Token Estimation Notes\n- Using approximation: 1 token ‚âà 4 characters (Gemini)\n- Actual token counts may vary\n- View Gemini API console for accurate counts\n"
        
        if filepath:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"üìä Token usage report exported to {filepath}")
        
        return report


class ScheduleAI:
    """LLM AI cho s·∫Øp x·∫øp l·ªãch h·ªçc"""
    
    def __init__(self):
        # Kh·ªüi t·∫°o client theo t√†i li·ªáu ch√≠nh th·ª©c
        self.client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        self.token_counter = TokenCounter()
        
        # System instruction cho SQL query generation
        self.sql_system_instruction = """B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ s·∫Øp x·∫øp th·ªùi kh√≥a bi·ªÉu cho tr∆∞·ªùng ƒë·∫°i h·ªçc v·ªõi kh·∫£ nƒÉng ƒë·ªçc v√† ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ CSDL_TKB (C∆° s·ªü d·ªØ li·ªáu Th·ªùi Kh√≥a Bi·ªÉu). 

**QUAN TR·ªåNG**: 
1. B·∫°n CH·ªà ƒë∆∞·ª£c ƒë∆∞a ra SQL queries. KH√îNG ƒë∆∞·ª£c t·ª± suy ƒëo√°n k·∫øt qu·∫£. 
2. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông th·ª±c thi queries v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ th·ª±c t·∫ø t·ª´ database.
3. Lu√¥n s·ª≠ d·ª•ng DISTINCT ƒë·ªÉ tr√°nh duplicate records.
4. Khi t√¨m ki·∫øm t√™n m√¥n h·ªçc, s·ª≠ d·ª•ng LIKE v·ªõi % ƒë·ªÉ t√¨m ki·∫øm g·∫ßn ƒë√∫ng.

==== C·∫§U TR√öC DATABASE TH·ª∞C T·∫æ ====
- tb_KHOA: Qu·∫£n l√Ω khoa (MaKhoa VARCHAR(12), TenKhoa NVARCHAR(200))
- tb_BO_MON: B·ªô m√¥n thu·ªôc khoa (MaBoMon VARCHAR(12), MaKhoa VARCHAR(12), TenBoMon NVARCHAR(200))
- tb_GIANG_VIEN: Gi·∫£ng vi√™n thu·ªôc b·ªô m√¥n (MaGV VARCHAR(12), MaBoMon VARCHAR(12), TenGV NVARCHAR(200), LoaiGV NVARCHAR(100), GhiChu NVARCHAR(300), Email VARCHAR(200))
- tb_DUKIEN_DT: D·ª± ki·∫øn ƒë√†o t·∫°o theo h·ªçc k·ª≥ (MaDuKienDT VARCHAR(15), NamHoc VARCHAR(9), HocKy TINYINT, NgayBD SMALLDATETIME, NgayKT SMALLDATETIME, MoTaHocKy NVARCHAR(100))
- tb_MON_HOC: M√¥n h·ªçc (MaMonHoc VARCHAR(10), TenMonHoc NVARCHAR(200), SoTinChi TINYINT, SoTietLT TINYINT, SoTietTH TINYINT, SoTuan TINYINT DEFAULT 15)
- tb_GV_DAY_MON: Gi·∫£ng vi√™n ƒë·ªß ƒëi·ªÅu ki·ªán d·∫°y m√¥n (MaMonHoc VARCHAR(10), MaGV VARCHAR(12))
- tb_KHUNG_TG: Khung th·ªùi gian c√°c ca (MaKhungGio TINYINT, TenCa NVARCHAR(50), GioBatDau TIME, GioKetThuc TIME, SoTiet TINYINT DEFAULT 3)
- tb_TIME_SLOT: Slot th·ªùi gian (TimeSlotID VARCHAR(10), Thu TINYINT 2-8, Ca TINYINT 1-5) - Thu t·ª´ 2-8 (T2-CN), Ca t·ª´ 1-5
- tb_PHONG_HOC: Ph√≤ng h·ªçc (MaPhong VARCHAR(12), LoaiPhong NVARCHAR(100), SucChua SMALLINT, ThietBi NVARCHAR(400), GhiChu NVARCHAR(200))
- tb_RANG_BUOC_MEM: R√†ng bu·ªôc m·ªÅm c√≥ tr·ªçng s·ªë (MaRangBuoc VARCHAR(15), TenRangBuoc NVARCHAR(200), MoTa NVARCHAR(500), TrongSo FLOAT)
- tb_LOP_MONHOC: L·ªõp m√¥n h·ªçc c·ª• th·ªÉ (MaLop VARCHAR(12), MaMonHoc VARCHAR(10), Nhom_MH TINYINT, To_MH TINYINT, SoLuongSV SMALLINT, HeDaoTao NVARCHAR(200), NgonNgu NVARCHAR(50), ThietBiYeuCau NVARCHAR(400), SoCaTuan TINYINT DEFAULT 1)
- tb_DOT_XEP: ƒê·ª£t x·∫øp th·ªùi kh√≥a bi·ªÉu (MaDot VARCHAR(20), MaDuKienDT VARCHAR(15), TenDot NVARCHAR(200), TrangThai VARCHAR(20): DRAFT/RUNNING/LOCKED/PUBLISHED, NgayTao DATETIME2, NgayKhoa DATETIME2)
- tb_PHAN_CONG: Ph√¢n c√¥ng gi·∫£ng vi√™n d·∫°y l·ªõp (MaDot VARCHAR(20), MaLop VARCHAR(12), MaGV VARCHAR(12))
- tb_RANG_BUOC_TRONG_DOT: √Åp d·ª•ng r√†ng bu·ªôc m·ªÅm trong ƒë·ª£t (MaDot VARCHAR(20), MaRangBuoc VARCHAR(15))
- tb_NGUYEN_VONG: Nguy·ªán v·ªçng gi·∫£ng vi√™n (MaGV VARCHAR(12), MaDot VARCHAR(20), TimeSlotID VARCHAR(10))
- tb_TKB: Th·ªùi kh√≥a bi·ªÉu ch√≠nh th·ª©c (MaTKB VARCHAR(15), MaDot VARCHAR(20), MaLop VARCHAR(12), MaPhong VARCHAR(12), TimeSlotID VARCHAR(10), TuanHoc VARCHAR(64), NgayBD SMALLDATETIME, NgayKT SMALLDATETIME)

==== H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI ====
1. Ph√¢n t√≠ch y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng
2. ƒê∆∞a ra SQL query ch√≠nh x√°c v√† ho√†n ch·ªânh
3. Gi·∫£i th√≠ch ng·∫Øn g·ªçn logic c·ªßa query
4. KH√îNG t·ª± suy ƒëo√°n k·∫øt qu·∫£ - ƒë·ªÉ h·ªá th·ªëng th·ª±c thi

**ƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:**
```sql
[SQL_QUERY_HERE]
```

**Gi·∫£i th√≠ch:** [M√¥ t·∫£ ng·∫Øn g·ªçn logic]

**L∆∞u √Ω:** H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông th·ª±c thi query v√† hi·ªÉn th·ªã k·∫øt qu·∫£ th·ª±c t·∫ø."""

        # System instruction cho schedule generation (hybrid optimized)
        self.schedule_system_instruction = """Task: Generate Class Schedule (JSON Output)
OUTPUT (JSON Only): {"schedule": [{"class": "...", "room": "...", "slot": "..."}]}

CRITICAL FORMATTING RULES
JSON Only: Output ONLY the JSON object. No explanations.
CLASS ID: Must be an EXACT COPY of the class.id (e.g., LOP-00000012).
SLOT FORMAT: Must STRICTLY adhere to T[2-7]-C[1-5] (e.g., T2-C1, T7-C5). T8 (Sunday) is NOT allowed.
COUNT: Total assignments in schedule MUST equal SUM(class.sessions).

INPUTS & CONTEXT
Inputs: classes (with sessions, type, students, equipment_required), rooms (LT/TH), teacher_constraints, soft_constraints.
Context: room_capacity[], room_type[], room_equipment[], teacher_preferences[].

SCHEDULING PRIORITIES (Strict Order)
PRIORITY 1: HARD CONSTRAINTS (MANDATORY)
Violation = Invalid Schedule.

HC-01 (Teacher Conflict): One teacher, one slot.
HC-02 (Room Conflict): One room, one slot.
HC-03 (Room Type): class.type ("LT"/"TH") MUST match room_type.
HC-04 (Capacity): room_capacity[room_id] MUST be >= class.students.
HC-05 (Equipment): room_equipment[room_id] MUST contain ALL class.equipment_required.
HC-06 (Teacher Busy): DO NOT schedule during teacher.busy_slots.
HC-07 (Teacher Limits): Respect max_slots_per_day and max_slots_per_week.
HC-08 (Session Rules - CRITICAL):
A class must have EXACTLY sessions assignments.
If sessions=2: MUST be a consecutive pair on the same day (e.g., T3-C1 & T3-C2, or T4-C3 & T4-C4).
If sessions=3: MUST be a consecutive trio on the same day (e.g., T5-C1, T5-C2, T5-C3).

Consecutive Rule: Valid groups are (C1,C2) and (C3,C4).
FORBIDDEN: Do not schedule across lunch (e.g., T2-C2 & T2-C3 is INVALID).

PRIORITY 2: TEACHER PREFERENCES (Semi-Hard)
Maximize assignments to teacher_preferences.preferred_slots.
ONLY violate if it conflicts with Priority 1 (Hard Constraints).
DO NOT violate preference to optimize Priority 3 or 4.

PRIORITY 3: TEACHER COMPACTNESS (Optimize for Teacher)
Goal: Minimize the number of days each teacher must come to campus.
Rule: Try to group all classes for a single teacher into the fewest days possible.
(Example: If a teacher has 3 classes, scheduling them on T2 and T3 is better than on T2, T4, and T5).
This is secondary to P1 (Hard Constraints) and P2 (Preferences).

PRIORITY 4: SCHOOL DISTRIBUTION & SOFT CONSTRAINTS (Low)
School Distribution: Spread the total school schedule load EVENLY across T2-T7 (Mon-Sat). Avoid concentrating >70% of all classes on 1-2 days. (This balances the school's resources).
Soft Constraints: After all above rules are met, optimize to minimize penalties based on soft_constraints weights (e.g., "Minimize Saturday")."""

    def generate_sql_query(self, user_prompt: str) -> str:
        """
        T·∫°o SQL query t·ª´ user prompt - S·ª≠ d·ª•ng cho chat/query
        Tr·∫£ v·ªÅ text/plain
        """
        config = types.GenerateContentConfig(
            temperature=0,
            top_p=0.95,
            top_k=64,
            max_output_tokens=8192,
            response_mime_type="text/plain",
            system_instruction=self.sql_system_instruction
        )
        
        response = self.client.models.generate_content(
            model='gemini-2.5-flash',
            contents=user_prompt,
            config=config
        )
        
        return response.text
    
    def generate_schedule_json(self, context_prompt: str) -> dict:
        """
        T·∫°o th·ªùi kh√≥a bi·ªÉu t·ª´ context - S·ª≠ d·ª•ng cho auto-scheduling
        Tr·∫£ v·ªÅ application/json
        
        Args:
            context_prompt: Full context string ch·ª©a:
                - Classes info (ma_lop, so_sv, so_ca_tuan, etc)
                - Rooms info (ma_phong, loai_phong, suc_chua)
                - TimeSlots info (time_slot_id, T2-C1, etc)
                - Teacher assignments (ma_gv)
                - Constraints & preferences
        
        Returns:
            Dict v·ªõi format: {"schedule": [{"class": "ma_lop", "room": "ma_phong", "slot": "T2-C1"}, ...]}
        """
        # üìä Log token usage stats TR∆Ø·ªöC request
        prompt_len = len(self.schedule_system_instruction)
        context_len = len(context_prompt)
        max_output_tokens = 50000  
        
        logger.info(f"üìä === TOKEN STATS (BEFORE REQUEST) ===")
        logger.info(f"   System Instruction: {prompt_len:,} chars (est. {prompt_len//4:,} tokens)")
        logger.info(f"   User Context: {context_len:,} chars (est. {context_len//4:,} tokens)")
        logger.info(f"   Combined Input: {prompt_len + context_len:,} chars (est. {(prompt_len + context_len)//4:,} tokens)")
        logger.info(f"   Max Output Tokens Requested: {max_output_tokens:,}")
        logger.warning(f"‚ö†Ô∏è  IMPORTANT: If response is still truncated, consider reducing context size or max_output_tokens")
        
        config = types.GenerateContentConfig(
            temperature=0.5,  # Cao h∆°n m·ªôt ch√∫t ƒë·ªÉ linh ho·∫°t trong scheduling
            top_p=0.95,
            top_k=40,
            max_output_tokens=max_output_tokens,  # TƒÉng cao ƒë·ªÉ ch·ª©a 216 schedules
            response_mime_type="application/json",  # Y√™u c·∫ßu tr·∫£ v·ªÅ JSON
            system_instruction=self.schedule_system_instruction
        )
        
        response = self.client.models.generate_content(
            model='gemini-2.5-flash',
            contents=context_prompt,
            config=config
        )
        
        # üî¥ CHECK: response.text is None?
        if response.text is None:
            logger.error(f"‚ùå CRITICAL: response.text is None!")
            logger.error(f"   Response object: {response}")
            logger.error(f"   Response candidates: {getattr(response, 'candidates', 'N/A')}")
            
            # Get finish_reason to understand why response is empty
            finish_reason = None
            try:
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    finish_reason = getattr(candidate, 'finish_reason', None)
                    finish_message = getattr(candidate, 'finish_message', None)
                    content = getattr(candidate, 'content', None)
                    
                    logger.error(f"   Candidates count: {len(response.candidates)}")
                    logger.error(f"   Finish reason: {finish_reason}")
                    logger.error(f"   Finish message: {finish_message}")
                    logger.error(f"   Content: {content}")
                    
                    if content:
                        logger.error(f"   Content parts: {getattr(content, 'parts', [])}")
            except Exception as e:
                logger.warning(f"   Could not extract candidate info: {e}")
            
            # Return error response with finish reason
            error_msg = f'LLM response.text is None'
            if finish_reason:
                error_msg += f' (finish_reason: {finish_reason})'
            
            fallback = {
                'schedule': [],
                'validation': {'errors': []},
                'metrics': {},
                'errors': [error_msg]
            }
            logger.warning(f"‚ö†Ô∏è Using fallback response due to None text")
            return fallback
            
        # üî¥ CHECK: response.text is empty string?
        if not response.text or response.text.strip() == '':
            logger.error(f"‚ùå CRITICAL: response.text is empty!")
            fallback = {
                'schedule': [],
                'validation': {'errors': []},
                'metrics': {},
                'errors': ['LLM response.text is empty']
            }
            logger.warning(f"‚ö†Ô∏è Using fallback response due to empty text")
            return fallback
            
        
        # ÔøΩüìä Log token usage stats SAU khi nh·∫≠n response
        response_len = len(response.text)
        usage_entry = self.token_counter.log_usage(
            prompt_len=prompt_len,
            context_len=context_len,
            max_output=max_output_tokens,
            response_len=response_len,
            model='gemini-2.5-flash'
        )
        
        logger.info(f"ÔøΩ === TOKEN STATS (AFTER RESPONSE) ===")
        logger.info(f"   Response Length: {response_len:,} chars (est. {response_len//4:,} tokens)")
        logger.info(f"   Total Input (Estimated): {usage_entry['estimated_input_tokens']:,} tokens")
        logger.info(f"   Total Output (Estimated): {usage_entry['estimated_output_tokens']:,} tokens")
        logger.info(f"   Total (Estimated): {usage_entry['total_estimated_tokens']:,} tokens")
        
        logger.info(f"üîç AI raw response length: {len(response.text)} chars")
        logger.info(f"üîç AI response preview (first 500 chars): {response.text[:500]}...")
        logger.info(f"üîç AI response suffix (last 200 chars): ...{response.text[-200:]}")
        
        # Check if response looks truncated (ends with incomplete JSON)
        if response.text.strip().endswith(',') or response.text.strip().endswith('[') or response.text.strip().endswith('{'):
            logger.warning(f"‚ö†Ô∏è TRUNCATED: Response ends with incomplete character!")
            logger.info(f"   Last 300 chars: {response.text[-300:]}")
            
            # Check finish_reason to confirm truncation
            try:
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    finish_reason = getattr(candidate, 'finish_reason', None)
                    logger.error(f"‚ùå Response was truncated! finish_reason: {finish_reason}")
                    logger.error(f"   Response length: {len(response.text)} chars (max was: 20000 tokens ‚âà 80000 chars)")
            except Exception as e:
                logger.debug(f"Could not check finish_reason: {e}")
        
        # Try to parse JSON - n·∫øu kh√¥ng th√†nh c√¥ng, try to extract JSON t·ª´ response
        try:
            parsed = json.loads(response.text)
            logger.info(f"‚úÖ Parsed JSON successfully. Keys: {list(parsed.keys())}")
            return parsed
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è Failed to parse JSON directly at position {e.pos}: {e.msg}")
            logger.info(f"üîç Response text around error (¬±100 chars): ...{response.text[max(0, e.pos-100):e.pos+100]}...")
            logger.info(f"üîç Response length: {len(response.text)} chars")
            logger.info(f"üîç Trying to extract JSON from response...")
            
            # Try to find JSON block in response - be more aggressive
            import re
            
            # Strategy: Try to find the main JSON object by looking for key patterns
            # 1. Try to find {"schedule": [...] as the main object start
            # 2. Try to balance braces to find valid JSON
            
            extracted_json = None
            
            # Method 1: Look for JSON starting with "schedule" key
            schedule_match = re.search(r'\{\s*"schedule"\s*:', response.text)
            if schedule_match:
                start_pos = schedule_match.start()
                logger.info(f"üîç Found 'schedule' key at position {start_pos}")
                
                # Try to extract from this position by counting braces
                brace_count = 0
                in_string = False
                escape_next = False
                end_pos = start_pos
                
                for i in range(start_pos, len(response.text)):
                    char = response.text[i]
                    
                    if escape_next:
                        escape_next = False
                        continue
                    
                    if char == '\\':
                        escape_next = True
                        continue
                    
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue
                    
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break
                
                if brace_count == 0:
                    json_str = response.text[start_pos:end_pos]
                    logger.info(f"üîç Extracted JSON by brace matching ({len(json_str)} chars)")
                    
                    try:
                        # Try to fix common JSON issues before parsing
                        # 1. Fix unterminated strings by finding incomplete quotes
                        json_str = re.sub(r',\s*}', '}', json_str)
                        json_str = re.sub(r',\s*]', ']', json_str)
                        
                        parsed = json.loads(json_str)
                        logger.info(f"‚úÖ Successfully extracted JSON via brace matching")
                        return parsed
                    except json.JSONDecodeError as e2:
                        logger.warning(f"‚ö†Ô∏è Brace matching extraction failed: {e2}")
                        extracted_json = None
            
            # Method 2: Try regex patterns if Method 1 failed
            if not extracted_json:
                patterns = [
                    (r'\{\s*"schedule"[\s\S]*\}(?=\s*(?:,|}|]|$))', 'schedule key pattern'),
                    (r'\{[^{}]*"schedule"[^{}]*\}', 'nested pattern'),
                ]
                
                for pattern, desc in patterns:
                    try:
                        json_match = re.search(pattern, response.text)
                        if json_match:
                            json_str = json_match.group(0)
                            logger.info(f"üîç Found potential JSON via {desc} ({len(json_str)} chars)")
                            
                            # Try to fix common JSON issues
                            json_str = re.sub(r',\s*}', '}', json_str)
                            json_str = re.sub(r',\s*]', ']', json_str)
                            
                            parsed = json.loads(json_str)
                            logger.info(f"‚úÖ Successfully extracted JSON via {desc}")
                            return parsed
                    except json.JSONDecodeError as e2:
                        logger.warning(f"‚ö†Ô∏è {desc} extraction failed: {e2}")
                        continue
            
            logger.error(f"‚ùå No valid JSON found in response after multiple extraction attempts")
            # Fallback: create empty schedule structure
            logger.warning(f"‚ö†Ô∏è Creating fallback schedule structure")
            fallback = {
                'schedule': [],
                'validation': {'errors': []},
                'metrics': {},
                'errors': [f'Failed to parse LLM response - Invalid JSON at position {e.pos}']
            }
            logger.info(f"üîç Using fallback response with {len(fallback['errors'])} errors")
            return fallback
    
    def _extract_sql_from_response(self, response_text: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c SQL queries t·ª´ response c·ªßa AI"""
        # T√¨m t·∫•t c·∫£ SQL queries trong code blocks
        sql_pattern = r'```sql\s*(.*?)\s*```'
        matches = re.findall(sql_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        # L√†m s·∫°ch v√† l·ªçc c√°c queries
        queries = []
        for match in matches:
            query = match.strip()
            
            # Ch·ªâ l√†m s·∫°ch c∆° b·∫£n, kh√¥ng s·ª≠a nhi·ªÅu ƒë·ªÉ tr√°nh l√†m h·ªèng SQL
            # 1. Lo·∫°i b·ªè comments
            query = re.sub(r'--.*$', '', query, flags=re.MULTILINE)
            query = re.sub(r'/\*.*?\*/', '', query, flags=re.DOTALL)
            
            # 2. X·ª≠ l√Ω ti·∫øng Vi·ªát - th√™m N prefix cho Unicode strings
            query = self._fix_vietnamese_strings(query)
            
            # 3. Ch·ªâ chu·∫©n h√≥a whitespace c∆° b·∫£n - thay nhi·ªÅu space/tab/newline th√†nh 1 space
            query = re.sub(r'\s+', ' ', query).strip()
            
            # 4. ƒê·∫£m b·∫£o k·∫øt th√∫c b·∫±ng d·∫•u ;
            if query and not query.endswith(';'):
                query = query + ';'
            
            if query and len(query) > 15:  # B·ªè qua queries qu√° ng·∫Øn
                queries.append(query)
                
        return queries
    
    def _fix_vietnamese_strings(self, query: str) -> str:
        """Th√™m N prefix cho c√°c chu·ªói ti·∫øng Vi·ªát trong SQL"""
        # T√¨m t·∫•t c·∫£ chu·ªói trong d·∫•u nh√°y ƒë∆°n
        def replace_vietnamese_string(match):
            string_content = match.group(1)
            
            # Ki·ªÉm tra xem chu·ªói c√≥ ch·ª©a k√Ω t·ª± ti·∫øng Vi·ªát kh√¥ng
            vietnamese_chars = '√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥'
            
            if any(char in vietnamese_chars for char in string_content):
                # Ki·ªÉm tra xem ƒë√£ c√≥ N prefix ch∆∞a
                full_match = match.group(0)
                if not full_match.startswith('N\'') and not full_match.startswith('n\''):
                    return f"N'{string_content}'"
                    
            return match.group(0)
        
        # T√¨m v√† thay th·∫ø c√°c chu·ªói trong d·∫•u nh√°y ƒë∆°n (kh√¥ng c√≥ N prefix)
        # Pattern n√†y t√¨m: 'string_content' nh∆∞ng kh√¥ng c√≥ N ho·∫∑c n ƒë·ª©ng tr∆∞·ªõc
        pattern = r"(?<!N)(?<!n)'([^']*?)'"
        query = re.sub(pattern, replace_vietnamese_string, query)
        
        return query

    def get_soft_constraints_prompt(self, ma_dot: str = None) -> str:
        """
        T·∫°o SQL query ƒë·ªÉ l·∫•y r√†ng bu·ªôc m·ªÅm cho ƒë·ª£t x·∫øp l·ªãch.
        ∆Øu ti√™n: tb_RANG_BUOC_TRONG_DOT > tb_RANG_BUOC_MEM (m·∫∑c ƒë·ªãnh)
        
        Args:
            ma_dot: M√£ ƒë·ª£t x·∫øp l·ªãch. N·∫øu None, ch·ªâ l·∫•y r√†ng bu·ªôc m·∫∑c ƒë·ªãnh
            
        Returns:
            SQL query string ƒë·ªÉ l·∫•y danh s√°ch r√†ng bu·ªôc m·ªÅm
        """
        if ma_dot:
            query = f"""
            -- L·∫•y r√†ng bu·ªôc m·ªÅm cho ƒë·ª£t {ma_dot}
            SELECT 
                rb.MaRangBuoc,
                rb.TenRangBuoc,
                rb.MoTa,
                rb.TrongSo
            FROM tb_RANG_BUOC_MEM rb
            INNER JOIN tb_RANG_BUOC_TRONG_DOT rbtd 
                ON rb.MaRangBuoc = rbtd.MaRangBuoc
            WHERE rbtd.MaDot = N'{ma_dot}'
            ORDER BY rb.TrongSo DESC;
            """
        else:
            query = """
            -- L·∫•y t·∫•t c·∫£ r√†ng bu·ªôc m·ªÅm m·∫∑c ƒë·ªãnh
            SELECT 
                MaRangBuoc,
                TenRangBuoc,
                MoTa,
                TrongSo
            FROM tb_RANG_BUOC_MEM
            ORDER BY TrongSo DESC;
            """
        
        return query.strip()
    
    def format_constraints_for_ai(self, constraints_data: List[dict]) -> str:
        """
        Format d·ªØ li·ªáu r√†ng bu·ªôc m·ªÅm t·ª´ SQL th√†nh chu·ªói cho AI context
        
        Args:
            constraints_data: List of dicts with keys: MaRangBuoc, TenRangBuoc, MoTa, TrongSo
            
        Returns:
            Formatted string for AI prompt
        """
        if not constraints_data:
            return "No soft constraints specified - use default optimization."
        
        lines = ["SOFT CONSTRAINTS (from database):"]
        for idx, c in enumerate(constraints_data, 1):
            lines.append(
                f"{idx}. {c['TenRangBuoc']} (weight={c['TrongSo']:.2f}): {c['MoTa']}"
            )
        
        return "\n".join(lines)
    
    def format_schedule_context_for_ai(self, prepared_data: dict) -> str:
        """
        Format d·ªØ li·ªáu scheduling t·ª´ prepare_data_for_llm() th√†nh context cho LLM
        
        Args:
            prepared_data: Dict t·ª´ schedule_generator_llm._prepare_data_for_llm()
                Ch·ª©a: rooms_by_type, timeslots, dot_xep_list, slot_mapping, etc
        
        Returns:
            Full context string ƒë·ªÉ ƒë∆∞a v√†o LLM prompt
        """
        lines = []
        
        # 1. Rooms format
        rooms_lt = prepared_data.get('rooms_by_type', {}).get('LT', [])
        rooms_th = prepared_data.get('rooms_by_type', {}).get('TH', [])
        
        lines.append("üè´ PH√íNG H·ªåC (Rooms):")
        lines.append(f"  LT (L√Ω Thuy·∫øt - Theory): {len(rooms_lt)} ph√≤ng")
        for room in rooms_lt[:5]:
            lines.append(f"    - {room['ma_phong']} (capacity: {room['suc_chua']})")
        if len(rooms_lt) > 5:
            lines.append(f"    ... and {len(rooms_lt) - 5} more")
        
        lines.append(f"  TH (Th·ª±c H√†nh - Practice): {len(rooms_th)} ph√≤ng")
        for room in rooms_th[:5]:
            lines.append(f"    - {room['ma_phong']} (capacity: {room['suc_chua']})")
        if len(rooms_th) > 5:
            lines.append(f"    ... and {len(rooms_th) - 5} more")
        
        # 2. TimeSlots format
        timeslots = prepared_data.get('timeslots', [])
        lines.append(f"\n‚è∞ TIME SLOTS: {len(timeslots)} slots")
        lines.append("  Format: T{day}-C{slot}")
        lines.append("  Days: T2-T7 (Monday-Saturday)")
        lines.append("  Slots: C1-C5 (periods)")
        for ts in timeslots[:10]:
            lines.append(f"    - {ts['id']}")
        if len(timeslots) > 10:
            lines.append(f"  ... and {len(timeslots) - 10} more")
        
        # 3. Classes & Teachers format
        total_classes = 0
        total_gv = set()
        total_sessions = 0
        
        for dot_info in prepared_data.get('dot_xep_list', []):
            classes = dot_info.get('phan_cong', [])  # FIX: D√πng 'phan_cong' thay v√¨ 'classes'
            total_classes += len(classes)
            for cls in classes:
                total_gv.add(cls.get('ma_gv'))
                total_sessions += cls.get('so_ca_tuan', 1)
        
        lines.append(f"\nüë• CLASSES & TEACHERS:")
        lines.append(f"  Total classes: {total_classes}")
        lines.append(f"  Total sessions to assign: {total_sessions}")
        lines.append(f"  Total teachers: {len(total_gv)}")
        
        # 4. Stats
        stats = prepared_data.get('stats', {})
        lines.append(f"\nüìä STATS:")
        lines.append(f"  Total rooms: {stats.get('total_rooms', 0)}")
        lines.append(f"  Total timeslots: {stats.get('total_timeslots', 0)}")
        
        # 5. Room Type & Capacity mapping for HC-05 & HC-04 validation
        lines.append(f"\nüè∑Ô∏è ROOM DETAILS (for constraint checking):")
        lines.append("  room_type: {")
        for room_type in ['LT', 'TH']:
            for room in prepared_data.get('rooms_by_type', {}).get(room_type, [])[:3]:
                lines.append(f'    "{room["ma_phong"]}": "{room_type}",')
        lines.append("    ... (all rooms)")
        lines.append("  }")
        
        lines.append("  room_capacity: {")
        for room_type in ['LT', 'TH']:
            for room in prepared_data.get('rooms_by_type', {}).get(room_type, [])[:3]:
                lines.append(f'    "{room["ma_phong"]}": {room["suc_chua"]},')
        lines.append("    ... (all rooms)")
        lines.append("  }")
        
        lines.append("  room_equipment: {")
        for room_type in ['LT', 'TH']:
            for room in prepared_data.get('rooms_by_type', {}).get(room_type, [])[:3]:
                equipment = room.get('thiet_bi', 'N/A')
                lines.append(f'    "{room["ma_phong"]}": "{equipment}",')
        lines.append("    ... (all rooms)")
        lines.append("  }")
        
        # 6. Class types for HC-05 validation
        lines.append(f"\nüìö CLASS TYPES (LT vs TH):")
        lt_count = 0
        th_count = 0
        for dot_info in prepared_data.get('dot_xep_list', []):
            for cls in dot_info.get('phan_cong', []):
                class_type = cls.get('loai_phong', 'LT')
                if class_type == 'TH':
                    th_count += 1
                else:
                    lt_count += 1
        lines.append(f"  LT classes: {lt_count}")
        lines.append(f"  TH classes: {th_count}")
        lines.append("  ‚ö†Ô∏è CRITICAL: TH classes MUST use TH rooms, LT classes MUST use LT rooms!")
        
        # 7. Teacher preferences (nguy·ªán v·ªçng GV)
        total_prefs = 0
        teacher_with_prefs = set()
        for dot_info in prepared_data.get('dot_xep_list', []):
            prefs = dot_info.get('preferences', [])
            total_prefs += len(prefs)
            for pref in prefs:
                teacher_with_prefs.add(pref.get('ma_gv'))
        
        if total_prefs > 0:
            lines.append(f"\nüíö TEACHER PREFERENCES:")
            lines.append(f"  Teachers with preferences: {len(teacher_with_prefs)}")
            lines.append(f"  Total preferred slots: {total_prefs}")
            lines.append("  Try to honor these when possible (soft constraint)")
        
        return "\n".join(lines)
    
    def export_token_report(self, filepath: str = None) -> str:
        """
        Export th·ªëng k√™ token usage ra markdown file
        
        Args:
            filepath: ƒê∆∞·ªùng d·∫´n file output (n·∫øu None, s·∫Ω l∆∞u v√†o output/token_usage_report.md)
        
        Returns:
            N·ªôi dung report d∆∞·ªõi d·∫°ng string
        """
        if filepath is None:
            filepath = os.path.join(os.path.dirname(__file__), '../../output/token_usage_report.md')
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        report = self.token_counter.export_report(filepath)
        logger.info(f"üìä Token usage report exported to {filepath}")
        return report
    
    def get_token_summary(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ token t√≥m t·∫Øt"""
        return self.token_counter.get_summary()