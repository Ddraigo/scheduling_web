"""
T·∫°o th·ªùi kh√≥a bi·ªÉu t·ªëi ∆∞u
"""

import json
import logging
import os
import sys
from datetime import datetime
from typing import Dict, List
import pandas as pd
from sqlalchemy import text
from tabulate import tabulate
from ..utils.helpers import json_serial

# ‚úÖ Import Google GenAI SDK (new official SDK)
from google import genai
from google.genai import types

# ‚úÖ Import Batch Scheduler cho LLM
from .batch_scheduler import BatchScheduler

# ‚úÖ Import Schedule Validator
from .schedule_validator import ScheduleValidator

# üî¥ Import Schedule Repair (NEW FIX)
from .schedule_repair import ScheduleRepair


def map_constraint_weights_from_sql(constraints_df: pd.DataFrame) -> Dict[str, float]:
    """
    Map r√†ng bu·ªôc m·ªÅm t·ª´ SQL (tb_RANG_BUOC_MEM) sang keys c·ªßa MetricsCalculator
    
    Args:
        constraints_df: DataFrame v·ªõi columns [TenRangBuoc, TrongSo]
        
    Returns:
        Dict v·ªõi keys: w_fair, w_wish, w_compact, w_unsat, w_daily_limit, w_compact_days
    """
    weights = {
        'w_fair': 1.0,           # Default fairness weight
        'w_wish': 1.2,           # Default wish satisfaction weight
        'w_compact': 0.5,        # Default compactness weight
        'w_unsat': 0.8,          # Default unmet wishes penalty
        'w_daily_limit': 0.6,    # Default daily limit compliance weight
        'w_compact_days': 0.4    # Default compact days weight
    }
    
    if constraints_df.empty:
        return weights
    
    # Map t·ª´ t√™n r√†ng bu·ªôc trong SQL sang keys
    # ‚ö†Ô∏è TH·ª® T·ª∞ QUAN TR·ªåNG: Specific patterns tr∆∞·ªõc, generic patterns sau
    name_mapping = [
        # RBM codes (specific - check first)
        ('rbm-001', 'w_daily_limit'),        # Gi·ªõi h·∫°n s·ªë ti·∫øt/ng√†y
        ('rbm-002', 'w_compact_days'),       # L·ªãch compact - √≠t ng√†y d·∫°y
        ('rbm-003', 'w_fair'),               # Ph√¢n c√¥ng c√¥ng b·∫±ng
        ('rbm-nguyen-vong', 'w_wish'),       # ∆Øu ti√™n nguy·ªán v·ªçng
        
        # Specific phrases (check before generic)
        ('∆∞u ti√™n nguy·ªán v·ªçng', 'w_wish'),
        ('gi·ªõi h·∫°n s·ªë ti·∫øt/ng√†y', 'w_daily_limit'),
        ('gi·ªõi h·∫°n ng√†y', 'w_daily_limit'),
        ('√≠t ng√†y', 'w_compact_days'),
        ('compact days', 'w_compact_days'),
        ('ph√¢n c√¥ng ƒë·ªÅu', 'w_fair'),
        
        # Generic keywords (check last)
        ('nguy·ªán v·ªçng', 'w_wish'),
        ('wish', 'w_wish'),
        ('c√¥ng b·∫±ng', 'w_fair'),
        ('fairness', 'w_fair'),
        ('g·ªçn', 'w_compact'),
        ('compact', 'w_compact'),          # Generic "compact" ‚Üí w_compact
        ('t·∫≠p trung', 'w_compact'),
        ('daily limit', 'w_daily_limit')
    ]
    
    for _, row in constraints_df.iterrows():
        ten_rb = str(row['TenRangBuoc']).lower()
        trong_so = float(row['TrongSo'])
        
        # T√¨m key t∆∞∆°ng ·ª©ng (first match wins)
        matched = False
        for keyword, weight_key in name_mapping:
            if keyword in ten_rb:
                weights[weight_key] = trong_so
                matched = True
                break  # Stop at first match
        
        if not matched:
            logger.warning(f"‚ö†Ô∏è Unknown constraint: '{row['TenRangBuoc']}' - ignored")
    
    # w_unsat lu√¥n b·∫±ng w_wish (penalty for unmet wishes)
    weights['w_unsat'] = weights['w_wish']
    
    return weights

logger = logging.getLogger(__name__)

# Import GA algorithm modules - CH·ªà KHI C·∫¶N D√ôNG
GA_AVAILABLE = False
ga_module = None
sql_to_teachers = None
sql_to_rooms = None
sql_to_courses = None
extract_soft_constraints_weights = None
ga_result_to_json = None

def _lazy_import_ga():
    """Lazy import GA module ƒë·ªÉ tr√°nh auto-run khi import"""
    global GA_AVAILABLE, ga_module
    global sql_to_teachers, sql_to_rooms, sql_to_courses
    global extract_soft_constraints_weights, ga_result_to_json
    
    if GA_AVAILABLE:
        return True
    
    try:
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'algorithm'))
        
        # ‚úÖ Import SQL-compatible version (no random data, no auto-run)
        import greedy_heuristic_ga_algorithm_sql as ga_mod
        
        from ga_adapter import (
            sql_to_teachers as stt, 
            sql_to_rooms as str_func, 
            sql_to_courses as stc,
            extract_soft_constraints_weights as escw, 
            ga_result_to_json as grtj
        )
        
        ga_module = ga_mod
        sql_to_teachers = stt
        sql_to_rooms = str_func
        sql_to_courses = stc
        extract_soft_constraints_weights = escw
        ga_result_to_json = grtj
        
        GA_AVAILABLE = True
        logger.info("‚úÖ GA algorithm (SQL version) loaded successfully")
        return True
    except Exception as e:
        logger.warning(f"GA algorithm kh√¥ng kh·∫£ d·ª•ng: {e}")
        import traceback
        traceback.print_exc()
        GA_AVAILABLE = False
        return False


class ScheduleGenerator:
    """T·∫°o th·ªùi kh√≥a bi·ªÉu t·ªëi ∆∞u"""
    
    def __init__(self, db_connection, ai_instance):
        self.db = db_connection
        self.ai = ai_instance
    
    def create_schedule_with_ga_directly(self, semester_code: str = '2025-2026_HK1') -> str:
        """
        Ch·∫°y TR·ª∞C TI·∫æP GA algorithm m√† KH√îNG d√πng AI
        
        Args:
            semester_code: M√£ h·ªçc k·ª≥ (m·∫∑c ƒë·ªãnh: '2025-2026_HK1')
            
        Returns:
            JSON string c·ªßa th·ªùi kh√≥a bi·ªÉu
        """
        print("üß¨ CH·∫†Y TR·ª∞C TI·∫æP GA ALGORITHM (B·ªé QUA AI)")
        print(f"üìÖ H·ªçc k·ª≥: {semester_code}")
        print("="*80)
        
        try:
            # G·ªçi tr·ª±c ti·∫øp GA algorithm
            schedule_json = self._create_schedule_with_ga_algorithm()
            
            # Parse v√† l∆∞u file
            result = json.loads(schedule_json)
            
            # L∆∞u v√†o file
            filename = f"schedule_ga_direct_{semester_code.replace('-', '_')}.json"
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            output_dir = os.path.join(project_root, 'output')
            os.makedirs(output_dir, exist_ok=True)
            filepath = os.path.join(output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=json_serial)
            
            print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {filepath}")
            
            # Format success message
            if 'schedule' in result:
                num_schedules = len(result['schedule'])
                metrics = result.get('metrics', {})
                
                return f"""
‚úÖ **T·∫†O TKB B·∫∞NG GA TH√ÄNH C√îNG!**

üìä K·∫øt qu·∫£:
- H·ªçc k·ª≥: {semester_code}
- ƒê√£ x·∫øp: {num_schedules} l·ªãch
- File: `{filename}`
- Fitness: {metrics.get('fitness_after', 'N/A')}
- Wish satisfaction: {metrics.get('wish_satisfaction', 'N/A')}

üìÅ ƒê√£ l∆∞u JSON ƒë·∫ßy ƒë·ªß v√†o: {filepath}
"""
            else:
                return schedule_json
                
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            return f"‚ùå L·ªói ch·∫°y GA: {e}\n\n{error_trace}"
    
    def create_optimal_schedule_to_json(self, semester_code: str, use_ga_directly: bool = False) -> str:
        """
        T·∫°o th·ªùi kh√≥a bi·ªÉu t·ªëi ∆∞u v√† l∆∞u v√†o file JSON
        
        Args:
            semester_code: M√£ h·ªçc k·ª≥
            use_ga_directly: True = ch·∫°y tr·ª±c ti·∫øp GA, b·ªè qua AI (m·∫∑c ƒë·ªãnh: False)
        """
        try:
            # ‚úÖ N·∫æU Y√äU C·∫¶U CH·∫†Y TR·ª∞C TI·∫æP GA
            if use_ga_directly:
                print("üß¨ Ch·∫ø ƒë·ªô: CH·∫†Y TR·ª∞C TI·∫æP GA (B·ªé QUA AI)")
                schedule_result = self._create_schedule_with_ga_algorithm()
                
                # Parse v√† validate
                final_result = json.loads(schedule_result)
                
                # L∆∞u file
                filename = f"schedule_ga_direct_{semester_code.replace('-', '_')}.json"
                current_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(os.path.dirname(current_dir))
                output_dir = os.path.join(project_root, 'output')
                os.makedirs(output_dir, exist_ok=True)
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(final_result, f, ensure_ascii=False, indent=2, default=json_serial)
                
                print(f"üíæ ƒê√£ l∆∞u th·ªùi kh√≥a bi·ªÉu v√†o: {filepath}")
                
                # Format message
                num_schedules = len(final_result.get('schedule', []))
                metrics = final_result.get('metrics', {})
                
                return f"""
‚úÖ **T·∫†O TKB B·∫∞NG GA TH√ÄNH C√îNG!**

üìä K·∫øt qu·∫£:
- H·ªçc k·ª≥: {semester_code}
- ƒê√£ x·∫øp: {num_schedules} l·ªãch
- File: `{filename}`
- Fitness: {metrics.get('fitness_after', 'N/A')}

üìÅ ƒê√£ l∆∞u v√†o: {filepath}
"""
            
            # ‚úÖ FLOW B√åN TH∆Ø·ªúNG: D√πng AI tr∆∞·ªõc
            print("ü§ñ ƒêang thu th·∫≠p d·ªØ li·ªáu ƒë·ªÉ t·∫°o l·ªãch t·ªëi ∆∞u...")
            
            # ƒê·ªãnh nghƒ©a c√°c queries
            queries = self._get_schedule_queries(semester_code)
            
            print(f"üìä ƒêang th·ª±c thi queries cho h·ªçc k·ª≥: {semester_code}...")
            
            # Th·ª±c thi queries v√† l·∫•y d·ªØ li·ªáu
            data_frames = self._execute_schedule_queries(queries, semester_code)
            
            if not self._validate_data(data_frames):
                return "‚ùå Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t·∫°o th·ªùi kh√≥a bi·ªÉu"
            
            print("üß† ƒêang s·ª≠ d·ª•ng AI ƒë·ªÉ t·∫°o l·ªãch t·ªëi ∆∞u...")
            
            # T·∫°o context cho AI
            scheduling_context = self._prepare_scheduling_context_for_json(
                semester_code, **data_frames
            )
            
            # G·ªçi AI ƒë·ªÉ t·∫°o th·ªùi kh√≥a bi·ªÉu JSON
            schedule_result = self._generate_optimal_schedule_json(scheduling_context)
            
            # üî¥ FIX: Apply schedule repair BEFORE validation
            print("üîß ƒêang s·ª≠a l·ªãch ƒë·ªÉ kh·∫Øc ph·ª•c vi ph·∫°m...")
            repair = ScheduleRepair()
            schedule_data = json.loads(schedule_result)
            schedule_list = schedule_data.get('schedule', [])
            
            repaired_schedule, repair_stats = repair.repair_schedule(
                schedule_list,
                data_frames['phan_cong_df'],
                data_frames['rooms_df'],
                data_frames['timeslots_df']['TimeSlotID'].tolist()
            )
            
            logger.info(f"‚úÖ Repair: {repair_stats}")
            schedule_data['schedule'] = repaired_schedule
            schedule_result = json.dumps(schedule_data, ensure_ascii=False, indent=2)
            
            # ‚úÖ VALIDATE v√† t√≠nh METRICS (on repaired schedule)
            print("üîç ƒêang validate l·ªãch h·ªçc v√† t√≠nh metrics...")
            validation_result = self._validate_and_calculate_metrics(
                schedule_result,
                data_frames
            )
            
            # ‚úÖ Th√™m metrics v√†o k·∫øt qu·∫£
            final_result = json.loads(schedule_result)
            final_result['validation'] = {
                'feasible': validation_result['feasible'],
                'all_assigned': validation_result['all_assigned'],
                'total_violations': validation_result['total_violations'],
                'violations_by_type': validation_result['violations_by_type']
            }
            final_result['metrics'] = validation_result['metrics']
            
            if not validation_result['feasible']:
                final_result['errors'] = validation_result['errors']
                print(f"‚ö†Ô∏è T√¨m th·∫•y {validation_result['total_violations']} vi ph·∫°m r√†ng bu·ªôc c·ª©ng!")
            else:
                print("‚úÖ L·ªãch h·ªçc th·ªèa m√£n t·∫•t c·∫£ r√†ng bu·ªôc c·ª©ng!")
            
            # Convert back to JSON string
            schedule_json = json.dumps(final_result, ensure_ascii=False, indent=2)
            
            # L∆∞u v√†o file trong th∆∞ m·ª•c output c·ªßa project
            filename = f"schedule_{semester_code.replace('-', '_')}.json"
            
            # L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c g·ªëc c·ªßa project (2 c·∫•p t·ª´ src/scheduling)
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            output_dir = os.path.join(project_root, 'output')
            
            # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
            os.makedirs(output_dir, exist_ok=True)
            
            filepath = os.path.join(output_dir, filename)
            
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(schedule_json)
                
                print(f"üíæ ƒê√£ l∆∞u th·ªùi kh√≥a bi·ªÉu v√†o: {filepath}")
                
                # Tr·∫£ v·ªÅ k·∫øt qu·∫£ v·ªõi metrics
                return self._format_success_message_with_metrics(
                    semester_code, filename, schedule_json, 
                    data_frames['phan_cong_df'],
                    validation_result
                )
                
            except Exception as e:
                return f"‚ùå L·ªói l∆∞u file: {e}\n\nüìÑ **K·∫øt qu·∫£ JSON:**\n{schedule_json[:1000]}..."
            
        except Exception as e:
            return f"‚ùå L·ªói t·∫°o th·ªùi kh√≥a bi·ªÉu: {e}"
    
    def _get_schedule_queries(self, semester_code: str) -> Dict[str, str]:
        """T·∫°o c√°c query t·ªëi ∆∞u - CH·ªà L·∫§Y D·ªÆ LI·ªÜU C·∫¶N THI·∫æT"""
        return {
            'dot_xep': f"""
                SELECT dx.MaDot, dx.TrangThai
                FROM tb_DOT_XEP dx 
                JOIN tb_DUKIEN_DT ddt ON dx.MaDuKienDT = ddt.MaDuKienDT
                WHERE dx.MaDuKienDT LIKE N'%{semester_code}%' 
                AND dx.TrangThai IN ('DRAFT', 'RUNNING')
            """,
            
            'phan_cong': f"""
                SELECT DISTINCT 
                    pc.MaLop, pc.MaGV,
                    lm.SoLuongSV, lm.SoCaTuan,
                    mh.MaMonHoc, mh.SoTinChi,
                    CASE 
                        WHEN mh.SoTietTH > 0 THEN N'TH'
                        ELSE N'LT'
                    END AS LoaiPhong
                FROM tb_PHAN_CONG pc
                JOIN tb_LOP_MONHOC lm ON pc.MaLop = lm.MaLop
                JOIN tb_MON_HOC mh ON lm.MaMonHoc = mh.MaMonHoc  
                JOIN tb_DOT_XEP dx ON pc.MaDot = dx.MaDot
                WHERE dx.MaDuKienDT LIKE N'%{semester_code}%'
            """,
            
            'rooms': """
                SELECT MaPhong, SucChua, LoaiPhong
                FROM tb_PHONG_HOC
                ORDER BY LoaiPhong, SucChua DESC
            """,
            
            'timeslots': """
                SELECT ts.TimeSlotID
                FROM tb_TIME_SLOT ts
                ORDER BY ts.Thu, ts.Ca
            """,
            
            'constraints': f"""
                SELECT rbm.TenRangBuoc, rbm.TrongSo
                FROM tb_RANG_BUOC_MEM rbm
                JOIN tb_RANG_BUOC_TRONG_DOT rbtd ON rbm.MaRangBuoc = rbtd.MaRangBuoc
                JOIN tb_DOT_XEP dx ON rbtd.MaDot = dx.MaDot
                WHERE dx.MaDuKienDT LIKE N'%{semester_code}%'
                ORDER BY rbm.TrongSo DESC
            """,
            
            'preferences': f"""
                SELECT nv.MaGV, nv.TimeSlotID
                FROM tb_NGUYEN_VONG nv
                JOIN tb_DOT_XEP dx ON nv.MaDot = dx.MaDot
                WHERE dx.MaDuKienDT LIKE N'%{semester_code}%'
            """
        }
    
    def _execute_schedule_queries(self, queries: Dict[str, str], semester_code: str) -> Dict[str, pd.DataFrame]:
        """Th·ª±c thi queries t·ªëi ∆∞u v√† tr·∫£ v·ªÅ DataFrames"""
        with self.db.engine.connect() as conn:
            result = {}
            
            # ƒê·ª£t x·∫øp
            data = conn.execute(text(queries['dot_xep'])).fetchall()
            result['dot_xep_df'] = pd.DataFrame(data, columns=['MaDot', 'TrangThai'])
            print(f"‚úÖ ƒê·ª£t x·∫øp: {len(data)} records")
            
            # Ph√¢n c√¥ng (CH·ªà 7 C·ªòT C·∫¶N THI·∫æT)
            data = conn.execute(text(queries['phan_cong'])).fetchall()
            result['phan_cong_df'] = pd.DataFrame(data, columns=[
                'MaLop', 'MaGV', 'SoLuongSV', 'SoCaTuan',
                'MaMonHoc', 'SoTinChi', 'LoaiPhong'
            ])
            print(f"‚úÖ Ph√¢n c√¥ng: {len(data)} records")
            
            # Ph√≤ng h·ªçc
            data = conn.execute(text(queries['rooms'])).fetchall()
            result['rooms_df'] = pd.DataFrame(data, columns=[
                'MaPhong', 'SucChua', 'LoaiPhong'
            ])
            print(f"‚úÖ Ph√≤ng h·ªçc: {len(data)} records")
            
            # Time slots (ch·ªâ c·∫ßn TimeSlotID)
            data = conn.execute(text(queries['timeslots'])).fetchall()
            result['timeslots_df'] = pd.DataFrame(data, columns=['TimeSlotID'])
            print(f"‚úÖ Time slots: {len(data)} records")
            
            # R√†ng bu·ªôc (ki·ªÉm tra ƒë·ª£t c√≥ r√†ng bu·ªôc kh√¥ng, n·∫øu kh√¥ng th√¨ l·∫•y m·∫∑c ƒë·ªãnh)
            check_constraints_query = text("""
                SELECT COUNT(*) as cnt
                FROM tb_RANG_BUOC_TRONG_DOT rbtd
                JOIN tb_DOT_XEP dx ON rbtd.MaDot = dx.MaDot
                WHERE dx.MaDuKienDT LIKE :semester_code
            """)
            check_result = conn.execute(check_constraints_query, {"semester_code": f"%{semester_code}%"}).fetchone()
            has_constraints = check_result[0] > 0 if check_result else False
            
            if has_constraints:
                # C√≥ r√†ng bu·ªôc trong ƒë·ª£t ‚Üí D√πng query ƒë√£ ƒë·ªãnh nghƒ©a
                data = conn.execute(text(queries['constraints'])).fetchall()
            else:
                # Kh√¥ng c√≥ r√†ng bu·ªôc trong ƒë·ª£t ‚Üí L·∫•y t·∫•t c·∫£ r√†ng bu·ªôc m·∫∑c ƒë·ªãnh
                default_constraints_query = text("""
                    SELECT TenRangBuoc, TrongSo
                    FROM tb_RANG_BUOC_MEM
                    ORDER BY TrongSo DESC
                """)
                data = conn.execute(default_constraints_query).fetchall()
            
            result['constraints_df'] = pd.DataFrame(data, columns=[
                'TenRangBuoc', 'TrongSo'
            ])
            print(f"‚úÖ R√†ng bu·ªôc: {len(data)} records" + (" (m·∫∑c ƒë·ªãnh)" if not has_constraints else " (t·ª´ ƒë·ª£t)"))
            
            # Nguy·ªán v·ªçng (CH·ªà M√É, KH√îNG C·∫¶N T√äN)
            data = conn.execute(text(queries['preferences'])).fetchall()
            result['preferences_df'] = pd.DataFrame(data, columns=[
                'MaGV', 'TimeSlotID'
            ])
            print(f"‚úÖ Nguy·ªán v·ªçng: {len(data)} records")
            
            return result
    
    def _validate_data(self, data_frames: Dict[str, pd.DataFrame]) -> bool:
        """Ki·ªÉm tra d·ªØ li·ªáu c√≥ ƒë·ªß kh√¥ng"""
        return not data_frames['phan_cong_df'].empty
    
    def _prepare_scheduling_context_for_json(
        self, semester_code: str, dot_xep_df, phan_cong_df, 
        rooms_df, timeslots_df, constraints_df, preferences_df
    ) -> str:
        """
        Chu·∫©n b·ªã context CHO AI - ENHANCED WITH FIXES
        
        üî¥ FIX #2: Add room capacity, full preferences, room type mapping
        üî¥ FIX #3: Add room capacity constraint
        üî¥ FIX #4: Include ALL 834 preferences (not just top 50)
        """
        
        # Create room capacity dict
        room_capacity_dict = {}
        for _, row in rooms_df.iterrows():
            room_capacity_dict[row['MaPhong']] = row['SucChua']
        
        # üî¥ FIX #2: Separate rooms by type with explicit mapping
        lt_rooms = rooms_df[rooms_df['LoaiPhong'].str.contains('thuy·∫øt|LT', case=False, na=False)]['MaPhong'].tolist()
        th_rooms = rooms_df[rooms_df['LoaiPhong'].str.contains('h√†nh|TH', case=False, na=False)]['MaPhong'].tolist()
        
        # Create room-type mapping
        room_type_map = {room: 'LT' for room in lt_rooms}
        room_type_map.update({room: 'TH' for room in th_rooms})
        
        # üî¥ FIX #4: Include ALL preferences, not just top 50
        # Group preferences by teacher (since preferences_df has one row per preference)
        preferences_by_teacher = {}
        if not preferences_df.empty:
            for _, row in preferences_df.iterrows():
                teacher_id = row['MaGV']
                time_slot = row['TimeSlotID']
                if teacher_id not in preferences_by_teacher:
                    preferences_by_teacher[teacher_id] = {'preferred': [], 'avoid': []}
                preferences_by_teacher[teacher_id]['preferred'].append(time_slot)
        
        preferences_list = [
            {
                'teacher': teacher_id,
                'preferred_slots': prefs['preferred'][:10],  # Top 10 preferred per teacher
                'total_preferences': len(prefs['preferred'])
            }
            for teacher_id, prefs in sorted(preferences_by_teacher.items())
        ]
        
        # ‚úÖ ENHANCED CONTEXT with ALL FIXES
        context = {
            'classes': [
                {
                    'id': row['MaLop'],
                    'teacher': row['MaGV'],
                    'students': row['SoLuongSV'],
                    'sessions': row['SoCaTuan'],
                    'type': row['LoaiPhong']  # LT or TH
                }
                for _, row in phan_cong_df.iterrows()
            ],
            
            'rooms': {
                'LT': lt_rooms,
                'TH': th_rooms
            },
            
            'timeslots': timeslots_df['TimeSlotID'].tolist(),
            
            # üî¥ FIX #2: Add room capacity constraint
            'room_capacity': room_capacity_dict,
            
            # üî¥ FIX #2: Add room type mapping (for HC-05/06 validation)
            'room_type': room_type_map,
            
            # üî¥ FIX #3: Add class capacity requirements
            'class_capacity_requirements': {
                row['MaLop']: row['SoLuongSV']
                for _, row in phan_cong_df.iterrows()
            },
            
            # üî¥ FIX #4: Include ALL teacher preferences (834 total)
            'teacher_preferences': preferences_list,
            'total_preferences_count': len(preferences_df) if not preferences_df.empty else 0
        }
        
        logger.info(f"üìä Context includes: {len(preferences_list)} teachers with {len(preferences_df) if not preferences_df.empty else 0} total preferences (was 50)")
        
        return json.dumps(context, ensure_ascii=False)
    
    def _generate_optimal_schedule_json(self, context: str) -> str:
        """
        AI t·∫°o TKB - S·ª≠ d·ª•ng ScheduleAI.generate_schedule_json()
        ‚≠ê CH·ªà G·ª¨I CONTEXT JSON, PROMPT ƒê√É C√ì TRONG schedule_ai.py (schedule_system_instruction)
        """
        
        # Parse context ƒë·ªÉ log
        context_data = json.loads(context)
        classes = context_data['classes']
        total_schedules = sum(c['sessions'] for c in classes)
        
        try:
            logger.info(f"ü§ñ Sending {len(classes)} classes ‚Üí {total_schedules} schedules")
            logger.info(f"üìã Using ScheduleAI system prompt (includes all 13 HC)")
            
            # ‚úÖ G·ª¨I CONTEXT JSON - Prompt ƒë√£ c√≥ trong ScheduleAI
            parsed = self.ai.generate_schedule_json(context)
            
            logger.info(f"üìù Response type: {type(parsed)}")
            
            # Re-parse context for validation
            context_data = json.loads(context)
            
            if 'schedule' in parsed:
                logger.info(f"üìä AI created {len(parsed['schedule'])} schedules")
                if len(parsed['schedule']) > 0:
                    logger.info(f"üîç Sample: {parsed['schedule'][0]}")
                    
                    # Validate
                    if self._validate_ai_schedule(parsed, context_data):
                        logger.info("‚úÖ AI single-shot success!")
                        return json.dumps(parsed, ensure_ascii=False, indent=2)
                else:
                    logger.warning(f"‚ö†Ô∏è AI returned EMPTY schedule!")
            
            # Failed ‚Üí try batching
            logger.warning("‚ö†Ô∏è Single-shot failed, try BATCHING...")
            return self._generate_schedule_with_batching(context_data)
                
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è JSON decode error: {e}, fallback sang GA")
            return self._create_schedule_with_ga_algorithm()
        except Exception as e:
            logger.error(f"‚ùå L·ªói AI: {e}")
            logger.warning("‚û°Ô∏è Fallback sang GA algorithm")
            return self._create_schedule_with_ga_algorithm()
    
    def _generate_schedule_with_batching(self, context_data: Dict) -> str:
        """
        ‚úÖ BATCH SCHEDULING - Chia nh·ªè task cho LLM
        """
        logger.info("üîÑ Starting BATCH SCHEDULING...")
        
        try:
            batch_scheduler = BatchScheduler(
                ai_instance=self.ai,
                batch_size=25  # 25 classes/batch ‚Üí ~9 batches
            )
            
            # Use new compact format
            classes = context_data['classes']
            rooms = context_data['rooms']
            timeslots = context_data['timeslots']
            
            result = batch_scheduler.generate_schedule_with_batching(
                classes=classes,
                rooms=rooms,
                timeslots=timeslots,
                max_retries=2
            )
            
            if 'error' in result:
                logger.error(f"‚ùå Batch scheduling FAILED: {result['error']}")
                logger.warning("‚û°Ô∏è Fallback sang GA algorithm")
                return self._create_schedule_with_ga_algorithm()
            
            # Success!
            logger.info(f"‚úÖ Batch scheduling SUCCESS: {len(result['schedule'])} schedules")
            return json.dumps(result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói batch scheduling: {e}")
            logger.warning("‚û°Ô∏è Fallback sang GA algorithm")
            return self._create_schedule_with_ga_algorithm()
    
    def _validate_ai_schedule(self, schedule_data: Dict, context_data: Dict) -> bool:
        """Validate AI output - CH·ªà 3 FIELDS B·∫ÆT BU·ªòC: class, room, slot"""
        try:
            if 'schedule' not in schedule_data:
                logger.warning("Missing 'schedule' key")
                return False
            
            schedule_list = schedule_data['schedule']
            if not isinstance(schedule_list, list) or len(schedule_list) == 0:
                logger.warning("Schedule empty or not a list")
                return False
            
            # ‚úÖ Valid IDs from context (new compact format)
            valid_rooms = set(context_data['rooms']['LT'] + context_data['rooms']['TH'])
            valid_timeslots = set(context_data['timeslots'])
            valid_classes = set(c['id'] for c in context_data['classes'])
            
            # üîç DEBUG: Log class IDs format
            logger.info(f"üîç Sample valid_classes (first 5): {list(valid_classes)[:5]}")
            logger.info(f"üîç Sample AI classes (first 5): {[s['class'] for s in schedule_list[:5]]}")

            
            # ‚úÖ Check 3 REQUIRED fields: class, room, slot (NO teacher - c√≥ trong tb_PHAN_CONG)
            required_keys = ['class', 'room', 'slot']
            invalid_count = 0
            
            for i, entry in enumerate(schedule_list[:50]):  # Check first 50
                # Missing keys?
                for key in required_keys:
                    if key not in entry:
                        logger.warning(f"Entry {i} missing '{key}'")
                        return False
                
                # ‚úÖ Validate real IDs from SQL
                if entry['class'] not in valid_classes:
                    logger.warning(f"Entry {i}: class '{entry['class']}' NOT EXIST!")
                    invalid_count += 1
                
                if entry['room'] not in valid_rooms:
                    logger.warning(f"Entry {i}: room '{entry['room']}' NOT EXIST!")
                    invalid_count += 1
                    
                if entry['slot'] not in valid_timeslots:
                    logger.warning(f"Entry {i}: slot '{entry['slot']}' NOT EXIST!")
                    invalid_count += 1
            
            # ‚ùå >20% invalid ‚Üí reject
            if invalid_count > len(schedule_list[:50]) * 0.2:
                logger.error(f"‚ùå VALIDATION FAILED: {invalid_count}/50 invalid IDs!")
                return False
            
            # Check minimum count (at least 50% of expected)
            expected_schedules = sum(c['sessions'] for c in context_data['classes'])
            min_schedules = expected_schedules * 0.5
            
            if len(schedule_list) < min_schedules:
                logger.warning(f"Only {len(schedule_list)}/{expected_schedules} schedules (< 50%)")
                return False
            
            logger.info(f"‚úÖ Validation passed: {len(schedule_list)} schedules, {invalid_count} invalid")
            return True
            
        except Exception as e:
            logger.error(f"Validation error: {e}")
            return False
    
    def _create_schedule_with_ga_algorithm(self) -> str:
        """
        S·ª≠ d·ª•ng Genetic Algorithm ƒë·ªÉ t·∫°o l·ªãch t·ªëi ∆∞u
        K·∫øt h·ª£p r√†ng bu·ªôc c·ª©ng + m·ªÅm t·ª´ SQL
        """
        # Lazy import GA modules
        if not _lazy_import_ga():
            error_msg = {
                "error": "GA algorithm kh√¥ng kh·∫£ d·ª•ng",
                "timestamp": datetime.now().isoformat(),
                "message": "Kh√¥ng th·ªÉ t·∫°o th·ªùi kh√≥a bi·ªÉu. Vui l√≤ng ki·ªÉm tra GA algorithm."
            }
            return json.dumps(error_msg, ensure_ascii=False, indent=2)
        
        try:
            print("üß¨ ƒêang s·ª≠ d·ª•ng Genetic Algorithm ƒë·ªÉ t·ªëi ∆∞u...")
            
            # === 1. QUERY D·ªÆ LI·ªÜU T·ª™ SQL ===
            
            # Gi·∫£ng vi√™n
            giang_vien_query = """
            SELECT DISTINCT gv.MaGV, gv.TenGV, gv.MaBoMon
            FROM tb_GIANG_VIEN gv
            JOIN tb_PHAN_CONG pc ON gv.MaGV = pc.MaGV
            WHERE pc.MaDot LIKE N'%2025-2026_HK1%'
            """
            giang_vien_df = self.db.execute_query(giang_vien_query)
            
            # Nguy·ªán v·ªçng
            nguyen_vong_query = """
            SELECT nv.MaGV, nv.TimeSlotID
            FROM tb_NGUYEN_VONG nv
            WHERE nv.MaDot LIKE N'%2025-2026_HK1%'
            """
            nguyen_vong_df = self.db.execute_query(nguyen_vong_query)
            
            # Ph√≤ng h·ªçc
            phong_hoc_query = """
            SELECT MaPhong, SucChua, LoaiPhong, ThietBi
            FROM tb_PHONG_HOC
            ORDER BY SucChua DESC
            """
            phong_hoc_df = self.db.execute_query(phong_hoc_query)
            
            # TimeSlots
            timeslots_query = """
            SELECT TimeSlotID, Thu, Ca
            FROM tb_TIME_SLOT
            WHERE Thu <> 8
            ORDER BY Thu, Ca
            """
            timeslots_df = self.db.execute_query(timeslots_query)
            
            # Ph√¢n c√¥ng
            phan_cong_query = """
            SELECT pc.MaDot, pc.MaLop, pc.MaGV
            FROM tb_PHAN_CONG pc
            WHERE pc.MaDot LIKE N'%2025-2026_HK1%'
            """
            phan_cong_df = self.db.execute_query(phan_cong_query)
            
            # L·ªõp m√¥n h·ªçc
            lop_monhoc_query = """
            SELECT lm.MaLop, lm.MaMonHoc, lm.SoLuongSV, lm.SoCaTuan, 
                   lm.ThietBiYeuCau, lm.HeDaoTao, lm.To_MH
            FROM tb_LOP_MONHOC lm
            JOIN tb_MON_HOC mh ON lm.MaMonHoc = mh.MaMonHoc
            WHERE lm.MaLop IN (SELECT MaLop FROM tb_PHAN_CONG WHERE MaDot LIKE N'%2025-2026_HK1%')
            """
            lop_monhoc_df = self.db.execute_query(lop_monhoc_query)
            
            # M√¥n h·ªçc
            mon_hoc_query = """
            SELECT mh.MaMonHoc, mh.TenMonHoc, mh.SoTinChi, mh.SoTietLT, mh.SoTietTH
            FROM tb_MON_HOC mh
            WHERE mh.MaMonHoc IN (SELECT MaMonHoc FROM tb_LOP_MONHOC WHERE MaLop IN 
                                 (SELECT MaLop FROM tb_PHAN_CONG WHERE MaDot LIKE N'%2025-2026_HK1%'))
            """
            mon_hoc_df = self.db.execute_query(mon_hoc_query)
            
            # R√†ng bu·ªôc m·ªÅm (∆∞u ti√™n t·ª´ ƒë·ª£t, fallback sang b·∫£ng chung)
            # B∆∞·ªõc 1: Ki·ªÉm tra xem c√≥ r√†ng bu·ªôc trong ƒë·ª£t kh√¥ng
            check_query = """
            SELECT COUNT(*) as cnt
            FROM tb_RANG_BUOC_TRONG_DOT rbtd
            WHERE rbtd.MaDot LIKE N'%2025-2026_HK1%'
            """
            check_result = self.db.execute_query(check_query)
            has_dot_constraints = check_result.iloc[0]['cnt'] > 0 if not check_result.empty else False
            
            # B∆∞·ªõc 2: L·∫•y r√†ng bu·ªôc t√πy theo c√≥ r√†ng bu·ªôc trong ƒë·ª£t hay kh√¥ng
            if has_dot_constraints:
                # C√≥ r√†ng bu·ªôc trong ƒë·ª£t ‚Üí CH·ªà l·∫•y nh·ªØng r√†ng bu·ªôc ƒë∆∞·ª£c g√°n cho ƒë·ª£t n√†y
                print("üìã S·ª≠ d·ª•ng r√†ng bu·ªôc m·ªÅm t·ª´ ƒë·ª£t x·∫øp...")
                rang_buoc_query = """
                SELECT rbm.MaRangBuoc, rbm.TenRangBuoc, rbm.TrongSo
                FROM tb_RANG_BUOC_MEM rbm
                INNER JOIN tb_RANG_BUOC_TRONG_DOT rbtd ON rbm.MaRangBuoc = rbtd.MaRangBuoc
                WHERE rbtd.MaDot LIKE N'%2025-2026_HK1%'
                """
            else:
                # KH√îNG c√≥ r√†ng bu·ªôc trong ƒë·ª£t ‚Üí L·∫•y T·∫§T C·∫¢ r√†ng bu·ªôc t·ª´ b·∫£ng chung l√†m m·∫∑c ƒë·ªãnh
                print("üìã ƒê·ª£t x·∫øp ch∆∞a c·∫•u h√¨nh r√†ng bu·ªôc ‚Üí S·ª≠ d·ª•ng T·∫§T C·∫¢ r√†ng bu·ªôc m·∫∑c ƒë·ªãnh t·ª´ tb_RANG_BUOC_MEM...")
                rang_buoc_query = """
                SELECT MaRangBuoc, TenRangBuoc, TrongSo
                FROM tb_RANG_BUOC_MEM
                ORDER BY TrongSo DESC
                """
            
            rang_buoc_df = self.db.execute_query(rang_buoc_query)
            
            print(f"‚úÖ ƒê√£ load: {len(giang_vien_df)} GV, {len(phong_hoc_df)} ph√≤ng, "
                  f"{len(phan_cong_df)} ph√¢n c√¥ng, {len(rang_buoc_df)} r√†ng bu·ªôc m·ªÅm")
            
            # === 2. CONVERT SANG C·∫§U TR√öC GA ===
            
            teachers = sql_to_teachers(giang_vien_df, nguyen_vong_df, timeslots_df, phan_cong_df)
            rooms = sql_to_rooms(phong_hoc_df)
            courses, mapping = sql_to_courses(phan_cong_df, lop_monhoc_df, mon_hoc_df, giang_vien_df)
            
            print(f"‚úÖ Converted: {len(teachers)} teachers, {len(rooms)} rooms, {len(courses)} course units")
            print(f"üîç DEBUG: V√†i t√™n courses ƒë·∫ßu ti√™n: {[c.name for c in courses[:5]]}")
            
            # === 3. C·∫§U H√åNH WEIGHTS T·ª™ R√ÄNG BU·ªòC M·ªÄM ===
            
            weights = extract_soft_constraints_weights(rang_buoc_df)
            # weights is now Dict[str, float] = {
            #     'w_daily_limit': 0.90, 'w_compact_days': 0.85,
            #     'w_fair': 1.0, 'w_wish': 1.2, 
            #     'w_compact': 0.5, 'w_unsat': 0.8
            # }
            
            print(f"üìä SQL Weights loaded: {weights}")
            
            # === 4. INJECT V√ÄO GA MODULE ===
            
            print("üîß Injecting SQL data v√†o GA module...")
            
            # 4.1. Inject main data structures
            ga_module.teachers = teachers
            ga_module.rooms = rooms
            ga_module.courses = courses
            
            # 4.2. ‚úÖ Inject SQL weights into sql_weights dict (used by fitness function)
            ga_module.sql_weights = weights
            
            # 4.3. Rebuild ALL global dictionaries
            print("üî® Rebuilding course_by_id...")
            ga_module.course_by_id = {c.id: c for c in courses}
            
            print("üî® Rebuilding assignments_by_teacher...")
            ga_module.assignments_by_teacher = {t.id: set() for t in teachers}
            
            print("üî® Rebuilding teacher_load, teacher_day_mask, dept_of_teacher...")
            ga_module.teacher_load = {t.id: 0 for t in teachers}
            ga_module.teacher_day_mask = {t.id: [0] * ga_module.DAYS for t in teachers}
            ga_module.dept_of_teacher = {t.id: t.dept for t in teachers}
            
            print("üî® Rebuilding teacher_week_slots, teacher_day_slots...")
            ga_module.teacher_week_slots = {t.id: 0 for t in teachers}
            ga_module.teacher_day_slots = {t.id: [0] * ga_module.DAYS for t in teachers}
            
            print("üî® Rebuilding assignment arrays...")
            ga_module.assign_teacher = {c.id: None for c in courses}
            ga_module.assign_day = {c.id: None for c in courses}
            ga_module.assign_slot = {c.id: None for c in courses}
            ga_module.assign_room = {c.id: None for c in courses}
            
            print("üî® Rebuilding candidate_rooms_for_course...")
            ga_module.candidate_rooms_for_course = {}
            for c in courses:
                room_ids = set()
                for r in rooms:
                    # Capacity check
                    if r.capacity < c.size:
                        continue
                    
                    # Room type check
                    if c.room_type_required and c.room_type_required.strip():
                        if c.room_type_required.lower() not in r.room_type.lower():
                            continue
                    
                    # Equipment check
                    if c.equipment_required and c.equipment_required.strip():
                        required_items = [item.strip().lower() for item in c.equipment_required.split(',') if item.strip()]
                        if not all(req_item in r.equipment.lower() for req_item in required_items):
                            continue
                    
                    room_ids.add(r.id)
                ga_module.candidate_rooms_for_course[c.id] = room_ids
            
            print("üî® Rebuilding feasible_slots...")
            ga_module.feasible_slots = {c.id: {} for c in courses}
            for c in courses:
                for tid in c.candidate_teachers:
                    f = []
                    t_bits = teachers[tid].availability_bits
                    for d in range(ga_module.DAYS):
                        for s in range(ga_module.SLOTS):
                            if ga_module.window_available(t_bits, d, s, c.duration):
                                f.append((d, s))
                    ga_module.feasible_slots[c.id][tid] = f
            
            print("üî® Calling build_option_lists() to rebuild OptionList v√† WishEnd...")
            ga_module.build_option_lists()
            
            print("‚úÖ Injection complete! GA module ready v·ªõi SQL data.")
            
            # === 5. CH·∫†Y GA ===
            
            print("üöÄ B·∫Øt ƒë·∫ßu ch·∫°y GA (Multi-start + Genetic Algorithm + Local Search)...")
            timetable, metrics = ga_module.run_demo()
            
            print(f"‚úÖ GA ho√†n th√†nh! Fitness={metrics.get('fitness_after', 'N/A')}, "
                  f"Wish satisfaction={metrics.get('wish_satisfaction', 0)}")
            
            # === 6. CONVERT K·∫æT QU·∫¢ V·ªÄ JSON ===
            
            result_json = ga_result_to_json(timetable, metrics, mapping, teachers, rooms)
            
            return json.dumps(result_json, ensure_ascii=False, indent=2, default=json_serial)
            
        except Exception as e:
            logger.error(f"L·ªói GA algorithm: {e}")
            import traceback
            traceback.print_exc()
            error_msg = {
                "error": f"GA algorithm th·∫•t b·∫°i: {e}",
                "timestamp": datetime.now().isoformat(),
                "message": "Kh√¥ng th·ªÉ t·∫°o th·ªùi kh√≥a bi·ªÉu. Vui l√≤ng ki·ªÉm tra d·ªØ li·ªáu ho·∫∑c c·∫•u h√¨nh GA."
            }
            return json.dumps(error_msg, ensure_ascii=False, indent=2, default=json_serial)
    
    def _format_success_message(
        self, semester_code: str, filename: str, 
        schedule_json: str, phan_cong_data
    ) -> str:
        """Th√¥ng b√°o th√†nh c√¥ng R√öT G·ªåN"""
        # ƒê·∫øm s·ªë l·ªãch ƒë√£ t·∫°o t·ª´ JSON
        try:
            data = json.loads(schedule_json)
            schedules_created = len(data.get('schedule', []))
        except:
            schedules_created = 0
        
        return f"""
‚úÖ **T·∫†O TKB TH√ÄNH C√îNG!**

üìä K·∫øt qu·∫£:
- H·ªçc k·ª≥: {semester_code}
- Ph√¢n c√¥ng: {len(phan_cong_data)} l·ªõp
- ƒê√£ x·∫øp: {schedules_created} l·ªãch
- File: `{filename}`

üìÅ ƒê√£ l∆∞u JSON ƒë·∫ßy ƒë·ªß v√†o file.
"""
    
    def _validate_and_calculate_metrics(
        self,
        schedule_json_str: str,
        data_frames: Dict[str, pd.DataFrame]
    ) -> Dict:
        """
        Validate l·ªãch h·ªçc v√† t√≠nh metrics
        
        Args:
            schedule_json_str: JSON string c·ªßa schedule
            data_frames: D·ªØ li·ªáu t·ª´ database
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ validation v√† metrics
        """
        try:
            # Parse JSON
            schedule_data = json.loads(schedule_json_str)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho validator
            classes_data = []
            for _, row in data_frames['phan_cong_df'].iterrows():
                classes_data.append({
                    'id': row['MaLop'],
                    'course': row['MaMonHoc'],
                    'students': row['SoLuongSV'],
                    'sessions': row['SoCaTuan'],
                    'type': row['LoaiPhong'],
                    'credits': row['SoTinChi']
                })
            
            # ‚≠ê Assignments data (tb_PHAN_CONG) - MaLop ‚Üí MaGV
            assignments_data = []
            for _, row in data_frames['phan_cong_df'].iterrows():
                assignments_data.append({
                    'MaLop': row['MaLop'],
                    'MaGV': row['MaGV']
                })
            
            # Rooms data
            rooms_df = data_frames['rooms_df']
            rooms_data = {
                'LT': rooms_df[rooms_df['LoaiPhong'].str.contains('thuy·∫øt|LT', case=False, na=False)]['MaPhong'].tolist(),
                'TH': rooms_df[rooms_df['LoaiPhong'].str.contains('h√†nh|TH', case=False, na=False)]['MaPhong'].tolist()
            }
            
            # Preferences data
            preferences_data = []
            if not data_frames['preferences_df'].empty:
                for teacher, group in data_frames['preferences_df'].groupby('MaGV'):
                    preferences_data.append({
                        'teacher': teacher,
                        'slots': group['TimeSlotID'].tolist()
                    })
            
            # ‚úÖ Constraints weights - MAP T·ª™ SQL SANG METRICS KEYS
            constraints_weights = map_constraint_weights_from_sql(data_frames['constraints_df'])
            logger.info(f"üìä Soft constraint weights from SQL: {constraints_weights}")
            
            # T·∫°o validator v√† validate
            validator = ScheduleValidator()
            result = validator.validate_schedule(
                schedule_data,
                classes_data,
                rooms_data,
                assignments_data,  # ‚≠ê Pass assignments t·ª´ tb_PHAN_CONG
                preferences_data,
                constraints_weights  # ‚≠ê Pass dynamic weights t·ª´ SQL
            )
            
            return result
            
        except Exception as e:
            logger.error(f"L·ªói validation: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "feasible": False,
                "errors": [f"Validation error: {str(e)}"],
                "total_violations": 1,
                "violations_by_type": {},
                "metrics": {},
                "all_assigned": False
            }
    
    def _format_success_message_with_metrics(
        self, semester_code: str, filename: str, 
        schedule_json: str, phan_cong_data,
        validation_result: Dict
    ) -> str:
        """Th√¥ng b√°o th√†nh c√¥ng v·ªõi metrics chi ti·∫øt"""
        # ƒê·∫øm s·ªë l·ªãch ƒë√£ t·∫°o t·ª´ JSON
        try:
            data = json.loads(schedule_json)
            schedules_created = len(data.get('schedule', []))
        except:
            schedules_created = 0
        
        metrics = validation_result.get('metrics', {})
        
        # Format metrics
        metrics_text = f"""
üìä **METRICS:**
- ‚úÖ Feasible: {validation_result.get('feasible', False)}
- ‚úÖ All Assigned: {validation_result.get('all_assigned', False)}
- üéØ Fitness: {metrics.get('fitness', 'N/A')}
- ‚öñÔ∏è Fairness (std): {metrics.get('fairness_std', 'N/A')}
- üíö Wish Satisfaction: {metrics.get('wish_satisfaction', 0)}/{metrics.get('wish_total', 0)} ({metrics.get('wish_coverage_rate', 0):.1%})
- üìÖ Compactness (gaps): {metrics.get('compactness_penalty', 'N/A')}
- üë• Teacher Load: min={metrics.get('teacher_load_min', 'N/A')}, max={metrics.get('teacher_load_max', 'N/A')}, avg={metrics.get('teacher_load_avg', 'N/A')}
"""
        
        if not validation_result.get('feasible'):
            violations = validation_result.get('violations_by_type', {})
            violations_text = "\n".join([f"  - {k}: {v} vi ph·∫°m" for k, v in violations.items()])
            metrics_text += f"\n‚ö†Ô∏è **VI PH·∫†M:**\n{violations_text}\n"
        
        return f"""
‚úÖ **T·∫†O TKB TH√ÄNH C√îNG!**

üìä K·∫øt qu·∫£:
- H·ªçc k·ª≥: {semester_code}
- Ph√¢n c√¥ng: {len(phan_cong_data)} l·ªõp
- ƒê√£ x·∫øp: {schedules_created} l·ªãch
- File: `{filename}`

{metrics_text}

üìÅ ƒê√£ l∆∞u JSON ƒë·∫ßy ƒë·ªß v√†o file.
"""

